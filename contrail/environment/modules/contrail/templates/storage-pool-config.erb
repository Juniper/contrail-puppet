#!/usr/bin/python
import sys
import subprocess
import os
from contrail_provisioning.storage.storagefs import ceph_utils

NUM_TARGET_OSD=<%=@contrail_storage_num_osd%>
storage_hostnames=['<%= @storage_compute_names%>']
storage_chassis_config=['<%= @contrail_pool_map%>']

osd_map_config=[]

if len(storage_hostnames) == 0:
    print 'storage_hostnames empty'
    sys.exit(1)

if len(storage_chassis_config) == 0:
    print 'storage_chassis_config is empty'
    sys.exit(1)


ceph_setup_utils = ceph_utils.SetupCephUtils()
NUM_CURR_OSD = ceph_setup_utils.exec_local("ceph -s | grep 'osdmap' | awk '{print $7}' ")

if int(NUM_CURR_OSD) == NUM_TARGET_OSD:
  osd_map_config=[]
  for host in storage_hostnames:
    file_name =host+'-disk-osd-map.txt'
    ceph_setup_utils.exec_local("rados -p internal get %s /tmp/%s" %(file_name, file_name))

    file=open('/tmp/'+file_name, 'r')
    osd_map_str=file.read()
    file.close()
    if osd_map_str != '' :
      osd_map_config = osd_map_config + eval(osd_map_str)
    #osd_map_config.append(osd_map_map)

  print osd_map_config

  # Initialize crush map
  crush_map = ceph_setup_utils.initialize_crush()
  # Do chassis configuration
  crush_map = ceph_setup_utils.do_pool_config(crush_map,
                                    storage_hostnames,
                                    storage_chassis_config,
                                    ['none'],
                                    osd_map_config)
  # Apply crushmap
  ceph_setup_utils.apply_crush(crush_map)

  # Configure Pools
  ceph_pool_list = ceph_setup_utils.do_configure_pools(
                                    storage_hostnames,
                                    storage_chassis_config,
                                    ['none'],
                                    ['none'], 'None')
  print ceph_pool_list

  #ceph_pool_list=['volumes_hdd_Pool_b', 'volumes_hdd_Pool_a', 'volumes_hdd_Pool_c']
  pool_details = {}
  for pool_name in ceph_pool_list:
    # Run local for storage-master for HDD/SSD pools
    ceph_setup_utils.exec_local('sudo ceph auth get-or-create client.%s mon \
				\'allow r\' osd \
				\'allow class-read object_prefix rbd_children, allow rwx pool=%s, allow rx pool=images\' \
				-o /etc/ceph/client.%s.keyring'
				%(pool_name, pool_name, pool_name))
    ceph_setup_utils.exec_local('sudo openstack-config --set /etc/ceph/ceph.conf client.%s keyring \
				/etc/ceph/client.%s.keyring'
				%(pool_name, pool_name))
    ceph_setup_utils.exec_local('ceph-authtool -p -n client.%s \
				/etc/ceph/client.%s.keyring > \
				/etc/ceph/client.%s'
				%(pool_name, pool_name, pool_name))

    # The following code with check if virsh secret for the pool is already present and sets if not

    secret_present = '0'
    line_num = 1
    while True:
      virsh_secret = ceph_setup_utils.exec_local('virsh secret-list  | \
					awk \'{print $1}\' | \
					awk \'NR > 2 { print }\' | \
					tail -n +%d | head -n 1'
					%(line_num))
      
      if virsh_secret != "":
        print virsh_secret
        #pdb.set_trace()
        cmd = 'virsh secret-dumpxml '+ virsh_secret + ' |  grep -w "client.' +  pool_name +'" | wc -l'

        print cmd
        secret_present = subprocess.check_output([cmd], shell=True)
        #secret_present = ceph_setup_utils.exec_local('virsh secret-dumpxml %s |  grep -w "client.%s" | wc -l'
					#%(virsh_secret.rstrip('\n'), pool_name))
	if secret_present != '0':
	   break
        else:
           break
        line_num += 1

    # If secret is not present, create new secret
    # Set the secret with the keyring
    if secret_present.rstrip('\n') == '0':
      ceph_setup_utils.exec_local('echo "<secret ephemeral=\'no\' private=\'no\'> \
			<usage type=\'ceph\'> \
			<name>client.%s secret</name> \
			</usage> \
			</secret>" > /tmp/secret_%s.xml'
			%(pool_name, pool_name))
      virsh_secret = ceph_setup_utils.exec_local('virsh secret-define --file \
			    /tmp/secret_%s.xml  2>&1 | \
			    cut -d " " -f 2' %(pool_name))
      #os.remove('/tmp/secret_' + pool_name + '.xml')

      volume_keyring_list = ceph_setup_utils.exec_local('cat /etc/ceph/client.%s.keyring | \
			    grep key' %(pool_name))
      volume_keyring = volume_keyring_list.split(' ')[2]

      cmd = 'virsh secret-set-value ' + virsh_secret.rstrip('\n') +' --base64 ' + volume_keyring.rstrip('\n')
      ceph_setup_utils.exec_local(cmd)
      
    pool_details[pool_name]= {'virsh_secret': virsh_secret.rstrip('\n')}

  print "\n"
  print pool_details

  for hostname in  storage_hostnames:
    file_name = 'pool_' + hostname +'.txt'
    file=open('/tmp/'+file_name,'w')
    file.write(str(pool_details))
    file.close()
    ceph_setup_utils.exec_local("rados -p internal put %s /tmp/%s" %(file_name, file_name))
    #os.remove('/tmp/'+file_name)

else:
  print 'Exiting as current OSDs={0}, needed= {1}'.format(NUM_CURR_OSD, NUM_TARGET_OSD)
  sys.exit (1)


